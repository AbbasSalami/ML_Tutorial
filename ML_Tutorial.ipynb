{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbbasSalami/ML_Tutorial/blob/main/ML_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9ImTiZmDMPn"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn import datasets\n",
        "import random\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, Dropout, BatchNormalization, Flatten, Conv2D, Conv1D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IRIS dataset (DT, SVM, MLP)"
      ],
      "metadata": {
        "id": "0EMLdfuTs4lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "att_label = [\"sepal_l\", \"sepal_w\", \"petal_l\", \"petal_w\"]\n",
        "data = pd.DataFrame(data = np.concatenate((iris[\"data\"], iris[\"target\"][:,np.newaxis]), axis = -1), columns = att_label + [\"species\"])\n",
        "# print(data.head())\n",
        "f1 = \"sepal_l\"\n",
        "f2 = \"sepal_w\"\n",
        "scatter = plt.scatter(data[f1], data[f2], c = data[\"species\"])\n",
        "plt.xlabel(f1)\n",
        "plt.ylabel(f2)"
      ],
      "metadata": {
        "id": "Rfj0Kr0Fs3fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = []\n",
        "# len(data)\n",
        "train = random.sample(range(0, len(data)-1), int(0.8*(len(data)-1)))\n",
        "# for idx in train:\n",
        "  "
      ],
      "metadata": {
        "id": "9P_iyb9d_uTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST dataset (SVM, MLP, CNN)"
      ],
      "metadata": {
        "id": "IHO5VVYTsqs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise 10 images\n",
        "def data_vis(input_data, L_idx):\n",
        "  fig, ax = plt.subplots(1,10, figsize=(12,2))\n",
        "  for i in L_idx:\n",
        "    ax[i].imshow(input_data[i], cmap='gray')\n",
        "    ax[i].set_xticks([])\n",
        "    ax[i].set_yticks([])\n",
        "\n",
        "def data_reshape(input_data):\n",
        "  return input_data.reshape(input_data.shape[0], input_data.shape[1]*input_data.shape[2])\n",
        "\n",
        "def show_shapes(*args):\n",
        "  for item in args:\n",
        "    print(item.shape)\n",
        "  print(\"\\n\")\n",
        "\n",
        "def one_hot(labels):\n",
        "  return np_utils.to_categorical(labels, len(np.unique(labels)))\n",
        "\n",
        "\n",
        "def training_vis(*args):\n",
        "  fig, ax = plt.subplots(1,len(args), figsize=(12,4))\n",
        "  for idx, item in enumerate(args):\n",
        "    ax[idx].plot(100*np.array(item.history['accuracy']), label = \"Train\")\n",
        "    ax[idx].plot(100*np.array(item.history['val_accuracy']), label = \"Validation\")\n",
        "    ax[idx].legend(loc = \"lower right\")\n",
        "    ax[idx].set_xlabel(\"Iterations\")\n",
        "    ax[idx].set_ylabel(\"Accuracy (%)\")\n",
        "\n",
        "\n",
        "def vis_misClass(model, input_test, label_test):\n",
        "  L_wrong = []\n",
        "  counter = 0\n",
        "  if len(y_test_hot.shape) == 1:\n",
        "    y_pred = model.predict(input_test)\n",
        "  else:\n",
        "    y_pred = np.argmax(model.predict(input_test),1)\n",
        "  \n",
        "  for i in range(len(y_pred)):\n",
        "    if label_test[i] == y_pred[i]:\n",
        "      counter += 1\n",
        "    else:\n",
        "      L_wrong.append(i)\n",
        "      \n",
        "  fig, ax = plt.subplots(1,10, figsize=(12,2))\n",
        "  for i in L_wrong[:10]:\n",
        "    ax[counter].imshow(x_test[i], cmap='gray')\n",
        "    ax[counter].set_xticks([])\n",
        "    ax[counter].set_yticks([])\n",
        "    counter += 1\n",
        "\n",
        "\n",
        "def vis_misClass(model, input_test, label_test, input_img):\n",
        "  if len(label_test.shape) == 1:\n",
        "    y_pred = model.predict(input_test)\n",
        "  else:\n",
        "    y_pred = np.argmax(model.predict(input_test),1)\n",
        "    label_test = np.argmax(label_test,1)\n",
        "  \n",
        "  L_wrong = []\n",
        "  counter = 0\n",
        "  for i in range(len(y_pred)):\n",
        "    if label_test[i] == y_pred[i]:\n",
        "      counter += 1\n",
        "    else:\n",
        "      L_wrong.append(i)\n",
        "      \n",
        "  k = min(len(L_wrong), 10)\n",
        "  L_wrong = random.sample(L_wrong, k)\n",
        "  counter = 0\n",
        "  fig, ax = plt.subplots(1,k, figsize=(12,2))\n",
        "  for i in L_wrong:\n",
        "    ax[counter].imshow(input_img[i], cmap='gray')\n",
        "    ax[counter].set_xticks([])\n",
        "    ax[counter].set_yticks([])\n",
        "    counter += 1"
      ],
      "metadata": {
        "id": "1JmpSwZ4rYK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e9hwlhZDdMK"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "data_vis(x_train, range(10))\n",
        "\n",
        "# Select a subset of data\n",
        "# x_train = x_train[:5000]\n",
        "# y_train = y_train[:5000]\n",
        "# x_test = x_test[:100]\n",
        "# y_test = y_test[:100]\n",
        "\n",
        "# Create train and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Normalising our data points\n",
        "x_train = x_train.astype('float32')/255\n",
        "x_val = x_val.astype('float32')/255\n",
        "x_test = x_test.astype('float32')/255\n",
        "\n",
        "# Reshape images to train conventional ML algorithm and MLP\n",
        "x_train_vec = data_reshape(x_train)\n",
        "x_val_vec = data_reshape(x_val)\n",
        "x_test_vec = data_reshape(x_test)\n",
        "\n",
        "# Reshape images to train CNN\n",
        "x_train_img = np.expand_dims(x_train, axis=3)\n",
        "x_val_img = np.expand_dims(x_val, axis=3)\n",
        "x_test_img = np.expand_dims(x_test, axis=3)\n",
        "\n",
        "# Showing shape of our data\n",
        "show_shapes(x_train_vec, x_train_img, y_train)\n",
        "\n",
        "# One-hot encoding\n",
        "y_train_hot = one_hot(y_train)\n",
        "y_val_hot = one_hot(y_val)\n",
        "y_test_hot = one_hot(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVwKSdUGIypO"
      },
      "outputs": [],
      "source": [
        "# Initialising support vector machine \n",
        "model_svm = svm.SVC()\n",
        "\n",
        "# Initialising multilayer perceptron\n",
        "Input_block = Input(shape = (x_train_vec.shape[-1]))\n",
        "out = Dense(512, activation = 'relu')(Input_block)\n",
        "out = Dense(256, activation = 'relu')(out)\n",
        "out = Dense(10, activation = 'softmax')(out)\n",
        "model_MLP = Model(inputs = Input_block, outputs = out)\n",
        "\n",
        "# Initialising convolutional neural network\n",
        "Input_block = Input(shape = (x_train_img.shape[1:]))\n",
        "out = Conv2D(32, kernel_size=(3, 3), activation='relu')(Input_block)\n",
        "out = MaxPooling2D(pool_size=(2, 2))(out)\n",
        "out = Flatten()(out)\n",
        "out = Dense(128, activation='relu')(out)\n",
        "out = Dropout(0.5)(out)\n",
        "out = Dense(10, activation = 'softmax')(out)\n",
        "model_CNN = Model(inputs = Input_block, outputs = out)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise MLP architecture\n",
        "print(\"\\MLP model\\n\")\n",
        "model_MLP.summary()\n",
        "plot_model(model_MLP, show_shapes=True)"
      ],
      "metadata": {
        "id": "NP82jxV3zsjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise CNN architecture\n",
        "print(\"\\nCNN model\\n\")\n",
        "model_CNN.summary()\n",
        "plot_model(model_CNN, show_shapes=True)"
      ],
      "metadata": {
        "id": "0Jig3wI60QSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training SVM\n",
        "print(\"Training a support vector machine ...\")\n",
        "model_svm.fit(x_train_vec, y_train)"
      ],
      "metadata": {
        "id": "NzVTAokU0Uu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training MLP\n",
        "print(\"Training a multilayer perceptron ...\")\n",
        "model_MLP.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "fittedModel_MLP = model_MLP.fit(x_train_vec, y_train_hot, batch_size = 32, epochs = 20, verbose = 1, validation_data=(x_val_vec, y_val_hot))"
      ],
      "metadata": {
        "id": "JhkvqW4r0mQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training CNN\n",
        "print(\"Training a convolutional neural network ...\")\n",
        "model_CNN.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "fittedModel_CNN = model_CNN.fit(x_train_img, y_train_hot, batch_size = 32, epochs = 20, verbose = 1, validation_data=(x_val_img, y_val_hot))"
      ],
      "metadata": {
        "id": "iWZ_yfkc0tTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualising the training process \n",
        "training_vis(fittedModel_MLP, fittedModel_CNN)"
      ],
      "metadata": {
        "id": "Ntf-oCix1uia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating models and comparison on the test data\n",
        "score_svm = model_svm.score(x_test_vec, y_test)\n",
        "score_MLP = model_MLP.evaluate(x_test_vec, y_test_hot, verbose=0)\n",
        "score_CNN = model_CNN.evaluate(x_test_img, y_test_hot, verbose=0)\n",
        "\n",
        "print(\"The SVM test accuracy:\", round(score_svm*100,1), \"%\")\n",
        "print(\"The MLP test accuracy:\", round(score_MLP[1]*100,1), \"%\")\n",
        "print(\"The CNN test accuracy:\", round(score_CNN[1]*100,1), \"%\")"
      ],
      "metadata": {
        "id": "wPaIv2Ck1uk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMisclassified samples by SVM\\n\")\n",
        "vis_misClass(model_svm, x_test_vec, y_test, x_test)"
      ],
      "metadata": {
        "id": "JfqmT-277SRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMisclassified samples by MLP\\n\")\n",
        "vis_misClass(model_MLP, x_test_vec, y_test_hot, x_test)"
      ],
      "metadata": {
        "id": "LWf5TbbVAoJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMisclassified samples by CNN\\n\")\n",
        "vis_misClass(model_CNN, x_test_img, y_test_hot, x_test)"
      ],
      "metadata": {
        "id": "_7o4dk_EAoTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLuUi0cm6fua"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6vwHchl33HSzSrXj6XvKl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}